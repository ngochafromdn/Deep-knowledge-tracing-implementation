{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4f4df586-d42e-49e4-9b77-232780575069",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting pandas\n",
      "  Downloading pandas-2.0.2-cp310-cp310-macosx_10_9_x86_64.whl (11.8 MB)\n",
      "\u001b[K     |████████████████████████████████| 11.8 MB 662 kB/s eta 0:00:01\n",
      "\u001b[?25hCollecting tzdata>=2022.1\n",
      "  Downloading tzdata-2023.3-py2.py3-none-any.whl (341 kB)\n",
      "\u001b[K     |████████████████████████████████| 341 kB 59.0 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: pytz>=2020.1 in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from pandas) (2022.1)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from pandas) (2.8.2)\n",
      "Requirement already satisfied: numpy>=1.21.0 in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from pandas) (1.22.3)\n",
      "Requirement already satisfied: six>=1.5 in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from python-dateutil>=2.8.2->pandas) (1.16.0)\n",
      "Installing collected packages: tzdata, pandas\n",
      "Successfully installed pandas-2.0.2 tzdata-2023.3\n",
      "\u001b[33mWARNING: You are using pip version 21.2.3; however, version 23.1.2 is available.\n",
      "You should consider upgrading via the '/usr/local/bin/python3 -m pip install --upgrade pip' command.\u001b[0m\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "fd89ea4e-13d8-4923-8aa2-9c9f3daed5fd",
   "metadata": {},
   "source": [
    "# Import data and data description"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d6650e50-e1a0-4a7f-bf30-006d25e4cb19",
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "Unable to import required dependencies:\nnumpy: \n\nIMPORTANT: PLEASE READ THIS FOR ADVICE ON HOW TO SOLVE THIS ISSUE!\n\nImporting the numpy C-extensions failed. This error can happen for\nmany reasons, often due to issues with your setup or how NumPy was\ninstalled.\n\nWe have compiled some common reasons and troubleshooting tips at:\n\n    https://numpy.org/devdocs/user/troubleshooting-importerror.html\n\nPlease note and check the following:\n\n  * The Python version is: Python3.10 from \"/usr/local/bin/python3\"\n  * The NumPy version is: \"1.22.3\"\n\nand make sure that they are the versions you expect.\nPlease carefully study the documentation linked above for further help.\n\nOriginal error was: dlopen(/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/numpy/core/_multiarray_umath.cpython-310-darwin.so, 0x0002): tried: '/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/numpy/core/_multiarray_umath.cpython-310-darwin.so' (mach-o file, but is an incompatible architecture (have 'arm64', need 'x86_64')), '/System/Volumes/Preboot/Cryptexes/OS/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/numpy/core/_multiarray_umath.cpython-310-darwin.so' (no such file), '/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/numpy/core/_multiarray_umath.cpython-310-darwin.so' (mach-o file, but is an incompatible architecture (have 'arm64', need 'x86_64'))\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[1;32m/Users/nguyenhoangngocha21/Documents/GitHub/Deep-knowledge-tracing-implementation/hihi.ipynb Cell 3\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> <a href='vscode-notebook-cell:/Users/nguyenhoangngocha21/Documents/GitHub/Deep-knowledge-tracing-implementation/hihi.ipynb#W2sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mpandas\u001b[39;00m \u001b[39mas\u001b[39;00m \u001b[39mpd\u001b[39;00m\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/nguyenhoangngocha21/Documents/GitHub/Deep-knowledge-tracing-implementation/hihi.ipynb#W2sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m data \u001b[39m=\u001b[39m pd\u001b[39m.\u001b[39mread_csv(\u001b[39m'\u001b[39m\u001b[39m2015_100_skill_builders_main_problems.csv\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/nguyenhoangngocha21/Documents/GitHub/Deep-knowledge-tracing-implementation/hihi.ipynb#W2sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m data\u001b[39m.\u001b[39mhead()\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/pandas/__init__.py:16\u001b[0m, in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     13\u001b[0m         _missing_dependencies\u001b[39m.\u001b[39mappend(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m{\u001b[39;00m_dependency\u001b[39m}\u001b[39;00m\u001b[39m: \u001b[39m\u001b[39m{\u001b[39;00m_e\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m)\n\u001b[1;32m     15\u001b[0m \u001b[39mif\u001b[39;00m _missing_dependencies:  \u001b[39m# pragma: no cover\u001b[39;00m\n\u001b[0;32m---> 16\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mImportError\u001b[39;00m(\n\u001b[1;32m     17\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mUnable to import required dependencies:\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m\"\u001b[39m \u001b[39m+\u001b[39m \u001b[39m\"\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m\"\u001b[39m\u001b[39m.\u001b[39mjoin(_missing_dependencies)\n\u001b[1;32m     18\u001b[0m     )\n\u001b[1;32m     19\u001b[0m \u001b[39mdel\u001b[39;00m _hard_dependencies, _dependency, _missing_dependencies\n\u001b[1;32m     21\u001b[0m \u001b[39m# numpy compat\u001b[39;00m\n",
      "\u001b[0;31mImportError\u001b[0m: Unable to import required dependencies:\nnumpy: \n\nIMPORTANT: PLEASE READ THIS FOR ADVICE ON HOW TO SOLVE THIS ISSUE!\n\nImporting the numpy C-extensions failed. This error can happen for\nmany reasons, often due to issues with your setup or how NumPy was\ninstalled.\n\nWe have compiled some common reasons and troubleshooting tips at:\n\n    https://numpy.org/devdocs/user/troubleshooting-importerror.html\n\nPlease note and check the following:\n\n  * The Python version is: Python3.10 from \"/usr/local/bin/python3\"\n  * The NumPy version is: \"1.22.3\"\n\nand make sure that they are the versions you expect.\nPlease carefully study the documentation linked above for further help.\n\nOriginal error was: dlopen(/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/numpy/core/_multiarray_umath.cpython-310-darwin.so, 0x0002): tried: '/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/numpy/core/_multiarray_umath.cpython-310-darwin.so' (mach-o file, but is an incompatible architecture (have 'arm64', need 'x86_64')), '/System/Volumes/Preboot/Cryptexes/OS/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/numpy/core/_multiarray_umath.cpython-310-darwin.so' (no such file), '/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/numpy/core/_multiarray_umath.cpython-310-darwin.so' (mach-o file, but is an incompatible architecture (have 'arm64', need 'x86_64'))\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "data = pd.read_csv('2015_100_skill_builders_main_problems.csv')\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7d4f9b34-5519-469f-98ae-de6c02ca0ce9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_descriptive_stats(data):\n",
    "    # Compute basic statistics\n",
    "    stats = data.describe()\n",
    "    \n",
    "    # Compute additional statistics\n",
    "    distinct_counts = data.nunique()\n",
    "    \n",
    "    return distinct_counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8ce50d8d-b0cf-495e-b5fd-3a5950d19db1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "user_id         19917\n",
       "log_id         708631\n",
       "sequence_id       100\n",
       "correct            11\n",
       "dtype: int64"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "compute_descriptive_stats(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8fc2ed5-44dd-4269-87e5-dc764262165f",
   "metadata": {},
   "source": [
    "# Data pre-processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ecb30a9e-c218-4518-aa09-6bbc20257c97",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>user_id</th>\n",
       "      <th>log_id</th>\n",
       "      <th>sequence_id</th>\n",
       "      <th>correct</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>50121</td>\n",
       "      <td>167478035</td>\n",
       "      <td>26</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>50121</td>\n",
       "      <td>167478043</td>\n",
       "      <td>26</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>50121</td>\n",
       "      <td>167478053</td>\n",
       "      <td>26</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>50121</td>\n",
       "      <td>167478069</td>\n",
       "      <td>26</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>50964</td>\n",
       "      <td>167478041</td>\n",
       "      <td>26</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   user_id     log_id  sequence_id  correct\n",
       "0    50121  167478035           26      0.0\n",
       "1    50121  167478043           26      1.0\n",
       "2    50121  167478053           26      1.0\n",
       "3    50121  167478069           26      1.0\n",
       "4    50964  167478041           26      1.0"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data['sequence_id'] = data['sequence_id'].rank(method='dense').astype(int)\n",
    "data.head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41e31bde-4e65-42f2-90a4-06a9be6a70b2",
   "metadata": {},
   "source": [
    "## Convert the raw data to train-data and test-data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9bfb53f4-8d07-404b-9f06-f4f234ef3e71",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "def read_data_from_csv(read_file, write_train, write_test):\n",
    "    df = pd.read_csv(read_file)\n",
    "    df['sequence_id'] = df['sequence_id'].rank(method='dense').astype(int)\n",
    "    ui_df = df.groupby(['user_id'], sort=False)\n",
    "    user_inters = []\n",
    "    for ui in ui_df:\n",
    "        tmp_inter = ui[1]\n",
    "        tmp_seq_len = len(tmp_inter)  # Ensure `len` is not reassigned\n",
    "        tmp_questions = [str(x) for x in list(tmp_inter['sequence_id'])]\n",
    "        tmp_ans = ['1' if x == 1.0 else '0' for x in list(tmp_inter['correct'])]\n",
    "        user_inters.append([str(tmp_seq_len), tmp_questions, tmp_ans])\n",
    "\n",
    "    train = user_inters[: int(0.8 * len(user_inters))]\n",
    "    test = user_inters[int(0.8 * len(user_inters)) :]\n",
    "    write_datafile(write_train, train)\n",
    "    write_datafile(write_test, test)\n",
    "    return\n",
    "\n",
    "def write_datafile(file, data):\n",
    "    with open(file, 'w') as f:\n",
    "        for index, dd in enumerate(data):\n",
    "            if index == 0:\n",
    "                continue  # Skip writing the header for the first row\n",
    "            row = [str(dd[0]), ','.join(dd[1]), ','.join(dd[2])]\n",
    "            for item in row:\n",
    "                f.write(item + '\\n')\n",
    "\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    read_data_from_csv('2015_100_skill_builders_main_problems.csv', 'Raw/train-data.csv', 'Raw/test-data.csv')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae2e41d2-6f81-40d1-afff-3599bb909f62",
   "metadata": {},
   "source": [
    "The provided code defines a class called RNN that represents a Recurrent Neural Network. The class has methods for building, training, and evaluating the RNN model. It uses linear layers, activation functions, and loss functions from the Torch library. The code also includes functions for saving and loading the model, calculating gradients, and performing forward propagation. The RNN class is initialized with parameters such as the number of questions, hidden units, dropout settings, and maximum steps. The code demonstrates how to create, train, and use an RNN model for sequential data analysis.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2ac96eb-5484-4fbd-8c83-707b8e7d7b2c",
   "metadata": {},
   "source": [
    "# Data loader and Read data function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29506984-3518-4045-8768-f7370300b289",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile Data/dataloader.py\n",
    "import torch\n",
    "import torch.utils.data as Data\n",
    "from Data.readdata import DataReader\n",
    "\n",
    "\n",
    "def getDataLoader(batch_size, num_of_questions, max_step):\n",
    "    handle = DataReader('Raw/train-data.csv',\n",
    "                        'Raw/test-data.csv', max_step,\n",
    "                        num_of_questions)\n",
    "    dtrain = torch.tensor(handle.getTrainData().astype(float).tolist(),\n",
    "                          dtype=torch.float32)\n",
    "    dtest = torch.tensor(handle.getTestData().astype(float).tolist(),\n",
    "                         dtype=torch.float32)\n",
    "    trainLoader = Data.DataLoader(dtrain, batch_size=batch_size, shuffle=True)\n",
    "    testLoader = Data.DataLoader(dtest, batch_size=batch_size, shuffle=False)\n",
    "    return trainLoader, testLoader\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f700c98a-b295-4124-9423-73b6040c51b8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting Data/readdata.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile Data/readdata.py\n",
    "\n",
    "\n",
    "import numpy as np\n",
    "import itertools\n",
    "\n",
    "\n",
    "class DataReader():\n",
    "    def __init__(self, train_path, test_path, maxstep, numofques):\n",
    "        self.train_path = 'Raw/train-data.csv'\n",
    "        self.test_path = 'Raw/test-data.csv'\n",
    "        self.maxstep = maxstep\n",
    "        self.numofques = numofques\n",
    "\n",
    "    def getData(self, file_path):\n",
    "        data = []\n",
    "        with open(file_path, 'r') as file:\n",
    "            for len, ques, ans in itertools.zip_longest(*[file] * 3):\n",
    "                len = int(len.strip().strip(','))\n",
    "                ques = [int(q) for q in ques.strip().strip(',').split(',')]\n",
    "                ans = [int(a) for a in ans.strip().strip(',').split(',')]\n",
    "                slices = len//self.maxstep + (1 if len % self.maxstep > 0 else 0)\n",
    "                for i in range(slices):\n",
    "                    temp = temp = np.zeros(shape=[self.maxstep, 2 * self.numofques])\n",
    "                    if len > 0:\n",
    "                        if len >= self.maxstep:\n",
    "                            steps = self.maxstep\n",
    "                        else:\n",
    "                            steps = len\n",
    "                        for j in range(steps):\n",
    "                            if ans[i*self.maxstep + j] == 1:\n",
    "                                temp[j][ques[i*self.maxstep + j]] = 1\n",
    "                            else:\n",
    "                                temp[j][ques[i*self.maxstep + j] + self.numofques] = 1\n",
    "                        len = len - self.maxstep\n",
    "                    data.append(temp.tolist())\n",
    "            print('done: ' + str(np.array(data).shape))\n",
    "        return data\n",
    "\n",
    "    def getTrainData(self):\n",
    "        print('loading train data...')\n",
    "        trainData = self.getData(self.train_path)\n",
    "        return np.array(trainData)\n",
    "\n",
    "    def getTestData(self):\n",
    "        print('loading test data...')\n",
    "        testData = self.getData(self.test_path)\n",
    "        return np.array(testData)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17170d06-1432-4a7c-8b1b-857f2fa3979c",
   "metadata": {},
   "source": [
    "Overall, this code trains an RNN model on the provided data and saves the trained model and training progress to files. It uses semi-sorted mini-batches for training and evaluates the model's accuracy on the test data.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "924127fd-b702-47e1-8cd5-feab5ef45695",
   "metadata": {},
   "source": [
    "# RNN Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "bf2c067f-5ee5-4a95-a5e8-fe9ab15bc15d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting model/DKT/RNNModel.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile model/DKT/RNNModel.py\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "class RNNModel(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim, layer_dim, output_dim, device):\n",
    "        super(RNNModel, self).__init__()\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.layer_dim = layer_dim\n",
    "        self.output_dim = output_dim\n",
    "        self.rnn = nn.RNN(input_dim, hidden_dim, layer_dim, batch_first=True, nonlinearity='tanh')\n",
    "        self.fc = nn.Linear(self.hidden_dim, self.output_dim)\n",
    "        self.sig = nn.Sigmoid()\n",
    "        self.device = device\n",
    "\n",
    "    def forward(self, x):\n",
    "        h0 = torch.zeros(self.layer_dim, x.size(0), self.hidden_dim, device=self.device)\n",
    "        out, hn = self.rnn(x, h0)\n",
    "        res = self.sig(self.fc(out))\n",
    "        return res\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2143e14c-c01a-41c7-a3e4-60b60479d63d",
   "metadata": {},
   "source": [
    "# Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "387ee00d-22bf-4ea1-80e1-9c1715b1c674",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "1. performance(ground_truth, prediction): Calculates and prints evaluation metrics such as AUC, F1 score, recall, and precision based on the ground truth labels and predicted probabilities.\n",
    "\n",
    "\n",
    "2. lossFunc(num_of_questions, max_step, device): Implements a custom loss function for the DKT model. Computes the loss, prediction, and ground truth tensors for evaluation.\n",
    "\n",
    "\n",
    "3. train_epoch(model, trainLoader, optimizer, loss_func, device): Performs a single training epoch. Computes forward pass, loss, backpropagation, and updates model parameters.\n",
    "\n",
    "\n",
    "4. test_epoch(model, testLoader, loss_func, device): Evaluates the DKT model on test data. Computes forward pass, loss, and collects predicted probabilities and ground truth labels. Prints evaluation metrics using performance function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c8f64a6-6fa6-4490-9a6d-ebb38eb8bf73",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile Evaluation/eval.py\n",
    "\n",
    "import tqdm\n",
    "import torch\n",
    "import logging\n",
    "\n",
    "import torch.nn as nn\n",
    "from sklearn import metrics\n",
    "\n",
    "logger = logging.getLogger('main.eval')\n",
    "\n",
    "\n",
    "def performance(ground_truth, prediction):\n",
    "    fpr, tpr, thresholds = metrics.roc_curve(ground_truth.detach().cpu().numpy(),\n",
    "                                             prediction.detach().cpu().numpy())\n",
    "    auc = metrics.auc(fpr, tpr)\n",
    "\n",
    "    f1 = metrics.f1_score(ground_truth.detach().cpu().numpy(),\n",
    "                          torch.round(prediction).detach().cpu().numpy())\n",
    "    recall = metrics.recall_score(ground_truth.detach().cpu().numpy(),\n",
    "                                  torch.round(prediction).detach().cpu().numpy())\n",
    "    precision = metrics.precision_score(\n",
    "        ground_truth.detach().cpu().numpy(),\n",
    "        torch.round(prediction).detach().cpu().numpy())\n",
    "    logger.info('auc: ' + str(auc) + ' f1: ' + str(f1) + ' recall: ' +\n",
    "                str(recall) + ' precision: ' + str(precision))\n",
    "    print('auc: ' + str(auc) + ' f1: ' + str(f1) + ' recall: ' + str(recall) +\n",
    "          ' precision: ' + str(precision))\n",
    "\n",
    "\n",
    "class lossFunc(nn.Module):\n",
    "    def __init__(self, num_of_questions, max_step, device):\n",
    "        super(lossFunc, self).__init__()\n",
    "        self.crossEntropy = nn.BCELoss()\n",
    "        self.num_of_questions = num_of_questions\n",
    "        self.max_step = max_step\n",
    "        self.device = device\n",
    "\n",
    "    def forward(self, pred, batch):\n",
    "        loss = 0\n",
    "        prediction = torch.tensor([], device=self.device)\n",
    "        ground_truth = torch.tensor([], device=self.device)\n",
    "        for student in range(pred.shape[0]):\n",
    "            delta = batch[student][:, 0:self.num_of_questions] + batch[\n",
    "                student][:, self.num_of_questions:]  # shape: [length, questions]\n",
    "            temp = pred[student][:self.max_step - 1].mm(delta[1:].t())\n",
    "            index = torch.tensor([[i for i in range(self.max_step - 1)]],\n",
    "                                 dtype=torch.long, device=self.device)\n",
    "            p = temp.gather(0, index)[0]\n",
    "            a = (((batch[student][:, 0:self.num_of_questions] -\n",
    "                   batch[student][:, self.num_of_questions:]).sum(1) + 1) //\n",
    "                 2)[1:]\n",
    "            for i in range(len(p) - 1, -1, -1):\n",
    "                if p[i] > 0:\n",
    "                    p = p[:i + 1]\n",
    "                    a = a[:i + 1]\n",
    "                    break\n",
    "            loss += self.crossEntropy(p, a)\n",
    "            prediction = torch.cat([prediction, p])\n",
    "            ground_truth = torch.cat([ground_truth, a])\n",
    "        return loss, prediction, ground_truth\n",
    "\n",
    "\n",
    "def train_epoch(model, trainLoader, optimizer, loss_func, device):\n",
    "    model.to(device)\n",
    "    for batch in tqdm.tqdm(trainLoader, desc='Training:    ', mininterval=2):\n",
    "        batch = batch.to(device)\n",
    "        pred = model(batch)\n",
    "        loss, prediction, ground_truth = loss_func(pred, batch)\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    return model, optimizer\n",
    "\n",
    "def test_epoch(model, testLoader, loss_func, device):\n",
    "    model.to(device)\n",
    "    ground_truth = torch.tensor([], device=device)\n",
    "    prediction = torch.tensor([], device=device)\n",
    "    for batch in tqdm.tqdm(testLoader, desc='Testing:     ', mininterval=2):\n",
    "        batch = batch.to(device)\n",
    "        pred = model(batch)\n",
    "        loss, p, a = loss_func(pred, batch)\n",
    "        prediction = torch.cat([prediction, p])\n",
    "        ground_truth = torch.cat([ground_truth, a])\n",
    "    performance(ground_truth, prediction)\n",
    "    return prediction\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b4880cc-9c2c-4e67-9cb0-9df26776d362",
   "metadata": {},
   "source": [
    "# Write main function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "4d05e54f-b45a-47d3-8b7b-322a3152ea52",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting run.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile run.py\n",
    "\n",
    "\"\"\"\n",
    "Usage:\n",
    "    run.py rnn --hidden=<h> [options]\n",
    "\n",
    "Options:\n",
    "    --length=<int>                      max length of question sequence [default: 50]\n",
    "    --questions=<int>                   num of question [default: 150]\n",
    "    --lr=<float>                        learning rate [default: 0.001]\n",
    "    --bs=<int>                          batch size [default: 64]\n",
    "    --seed=<int>                        random seed [default: 13]\n",
    "    --epochs=<int>                      number of epochs [default: 2]\n",
    "    --cuda=<int>                        use GPU id [default: 0]\n",
    "    --hidden=<int>                      dimension of hidden state [default: 50]\n",
    "    --layers=<int>                      layers of rnn [default: 4]\n",
    "    --dropout=<float>                   dropout rate [default: 0.1]\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "import os\n",
    "import random\n",
    "import logging\n",
    "import torch\n",
    "\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "\n",
    "from datetime import datetime\n",
    "from docopt import docopt\n",
    "from Data.dataloader import getDataLoader\n",
    "from Evaluation import eval\n",
    "\n",
    "\n",
    "def setup_seed(seed=0):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.random.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "\n",
    "\n",
    "def main():\n",
    "\n",
    "    args = docopt(__doc__)\n",
    "    length = int(args['--length'])\n",
    "    questions = int(args['--questions'])\n",
    "    lr = float(args['--lr'])\n",
    "    bs = int(args['--bs'])\n",
    "    seed = int(args['--seed'])\n",
    "    epochs = int(args['--epochs'])\n",
    "    cuda = args['--cuda']\n",
    "    hidden = int(args['--hidden'])\n",
    "    layers = int(args['--layers'])\n",
    "    dropout = float(args['--dropout'])\n",
    "    model_type = 'RNN'\n",
    "\n",
    "    logger = logging.getLogger('main')\n",
    "    logger.setLevel(level=logging.DEBUG)\n",
    "    date = datetime.now()\n",
    "    setup_seed(seed)\n",
    "\n",
    "    if torch.cuda.is_available():\n",
    "        os.environ[\"CUDA_VISIBLE_DEVICES\"] = cuda\n",
    "        device = torch.device('cuda')\n",
    "    else:\n",
    "        device = torch.device('cpu')\n",
    "\n",
    "    trainLoader, testLoader = getDataLoader(bs, questions, length)\n",
    "\n",
    "    from model.DKT.RNNModel import RNNModel\n",
    "    model = RNNModel(questions * 2, hidden, layers, questions, device)\n",
    "\n",
    "    optimizer = optim.Adam(model.parameters(), lr=lr)\n",
    "    loss_func = eval.lossFunc(questions, length, device)\n",
    "\n",
    "    predicted_var = None  # Initialize a variable to store the predicted variable\n",
    "\n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "        print('epoch: ' + str(epoch))\n",
    "        model, optimizer = eval.train_epoch(model, trainLoader, optimizer,\n",
    "                                          loss_func, device)\n",
    "        logger.info(f'epoch {epoch}')\n",
    "\n",
    "    # Save the model\n",
    "    model_dir = 'Result'  \n",
    "    os.makedirs(model_dir, exist_ok=True)\n",
    "    model_path = os.path.join(model_dir, 'model.pth')\n",
    "    torch.save(model.state_dict(), model_path)\n",
    "    \n",
    "\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    main()\n",
    "\n",
    "\n",
    "   \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "01f93f0e-f631-4796-b390-dca1225485c8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading train data...\n",
      "done: (22037, 50, 300)\n",
      "loading test data...\n",
      "done: (4691, 50, 300)\n",
      "epoch: 0\n",
      "Training:    : 100%|██████████████████████████| 345/345 [00:17<00:00, 19.93it/s]\n",
      "epoch: 1\n",
      "Training:    : 100%|██████████████████████████| 345/345 [00:17<00:00, 20.11it/s]\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/nguyenhoangngocha21/Documents/GitHub/Knowledge-tracing-model/run.py\", line 96, in <module>\n",
      "    main()\n",
      "  File \"/Users/nguyenhoangngocha21/Documents/GitHub/Knowledge-tracing-model/run.py\", line 91, in main\n",
      "    return predicted_values\n",
      "NameError: name 'predicted_values' is not defined\n"
     ]
    }
   ],
   "source": [
    "!python run.py rnn --hidden=50\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b663af11-4b3e-481e-926d-37fa8ce48ca3",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile Web.py \n",
    "import streamlit as st\n",
    "import torch\n",
    "from torch.optim import Adam\n",
    "from model.DKT.RNNModel import RNNModel\n",
    "from Data.dataloader import getDataLoader\n",
    "from Evaluation import eval\n",
    "\n",
    "def main():\n",
    "    st.title('Knowledge Tracing Model')\n",
    "\n",
    "    # Description\n",
    "    st.markdown(\"\"\"\n",
    "        ## Welcome to the Knowledge Tracing Model\n",
    "\n",
    "        This web application allows you to use a pre-trained Knowledge Tracing Model to make predictions on test data.\n",
    "\n",
    "        ### Model Parameters\n",
    "\n",
    "        Use the sidebar to customize the model's architecture and hyperparameters.\n",
    "\n",
    "        - **Batch Size:** Configure the batch size for training and prediction.\n",
    "        - **Number of Epochs:** Set the number of training epochs.\n",
    "\n",
    "        ### Load Data\n",
    "\n",
    "        Click the 'Load Data' button to load the training and test data. The data will be loaded using the specified batch size.\n",
    "\n",
    "        ### Run Model\n",
    "\n",
    "        Once the data is loaded (Data loaded successfully!), click the 'Run Model' button to start the prediction on the test data. The model will make predictions and display the results.\n",
    "\n",
    "        ---\n",
    "    \"\"\")\n",
    "\n",
    "    # Parameters\n",
    "    input_dim = 300\n",
    "    hidden_dim = 50\n",
    "    layer_dim = 4\n",
    "    output_dim = 150\n",
    "    batch_size = st.sidebar.number_input('Batch Size', min_value=1, step=1, value=64)\n",
    "    num_epochs = st.sidebar.number_input('Number of Epochs', min_value=1, step=1, value=10)\n",
    "\n",
    "    # Load the trained model\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    model = RNNModel(input_dim, hidden_dim, layer_dim, output_dim, device)\n",
    "    model.load_state_dict(torch.load('Result/model.pth'))\n",
    "    model.to(device)\n",
    "\n",
    "    # Load the data\n",
    "    train_loader, test_loader = None, None\n",
    "\n",
    "    if st.sidebar.button('Load Data'):\n",
    "        with st.spinner(\"Loading data...\"):\n",
    "            train_loader, test_loader = getDataLoader(batch_size, output_dim, hidden_dim)\n",
    "        st.success(\"Data loaded successfully!\")\n",
    "\n",
    "    # Perform prediction on the test data and display the results\n",
    "    if st.sidebar.button('Run Model') and train_loader is not None and test_loader is not None:\n",
    "        prediction = eval.test_epoch(model, test_loader, eval.lossFunc(output_dim, hidden_dim, device), device)\n",
    "        st.markdown(\"<h3>Prediction results:</h3>\", unsafe_allow_html=True)\n",
    "        st.markdown(f\"<p style='font-size: 18px;'>{prediction}</p>\", unsafe_allow_html=True)\n",
    "\n",
    "    # Training loop\n",
    "    if train_loader is not None:\n",
    "        optimizer = Adam(model.parameters())  # Initialize optimizer\n",
    "        loss_func = eval.lossFunc(output_dim, hidden_dim, device)  # Define the loss function\n",
    "        st.write(\" ### Training progress:\")\n",
    "        for epoch in range(num_epochs):\n",
    "            # Perform training\n",
    "            model, optimizer = eval.train_epoch(model, train_loader, optimizer, loss_func, device)\n",
    "\n",
    "            # Display epoch information on Streamlit\n",
    "            st.write(f\"Epoch {epoch+1} completed\")\n",
    "\n",
    "            # Perform prediction on the test data and display the results after each epoch\n",
    "            prediction = eval.test_epoch(model, test_loader, eval.lossFunc(output_dim, hidden_dim, device), device)\n",
    "            st.markdown(f\"<p style='font-size: 18px;'>{prediction}</p>\", unsafe_allow_html=True)\n",
    "             \n",
    "if __name__ == '__main__':\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "702db053-4017-4e29-98f1-7e7d30d1f572",
   "metadata": {},
   "outputs": [],
   "source": [
    "!Streamlit run Web.py --\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc730d27-fc2b-497d-84ab-202fc39c55f1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
